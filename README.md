# ğŸ˜ Shared PostgreSQL Backup System

Cloud-first PostgreSQL backup and restore system with automated daily backups, Google Drive integration, and zero local storage.

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [File Structure](#-file-structure)
- [Troubleshooting](#-troubleshooting)
- [Advanced](#-advanced)

---

## âœ¨ Features

- **ğŸ”„ Automated Daily Backups**: Scheduled at 02:00 AM TRT
- **â˜ï¸ Cloud-First Architecture**: All backups stored in Google Drive (S3 ready)
- **ğŸ—„ï¸ Multiple Databases**: Each database backed up to separate SQL files
- **ğŸ”’ Safety Backups**: Automatic safety dump before restore operations
- **ğŸ§¹ Auto Cleanup**: 15-day retention policy on cloud storage
- **ğŸ“… Hierarchical Storage**: Year/Month/Day folder structure
- **â™»ï¸ Smart Archiving**: Multiple same-day backups archived in `olds/HH_MM/` subfolders
- **ğŸš€ Zero Local Storage**: Temp files only, auto-cleaned after operations
- **ğŸ“Š pgAdmin Integration**: Web UI for database management

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Compose Stack                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Postgres   â”‚  â”‚   pgAdmin    â”‚  â”‚   pgBackup   â”‚    â”‚
â”‚  â”‚   (Port 5432)â”‚â—„â”€â”¤  (Port 9090) â”‚  â”‚   (Cron)     â”‚    â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                                     â”‚            â”‚
â”‚         â”‚                                     â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                     â”‚
          â”‚                                     â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚   Volume   â”‚                        â”‚  Rclone  â”‚
    â”‚ postgres_  â”‚                        â”‚  Config  â”‚
    â”‚   data     â”‚                        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
                                               â”‚
                                         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                         â”‚   Google   â”‚
                                         â”‚   Drive    â”‚
                                         â”‚            â”‚
                                         â”‚  records/  â”‚
                                         â”‚  manual_   â”‚
                                         â”‚  backups/  â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backup Flow

```
1. Cron triggers backup.py (02:00 AM daily)
2. Create temp directory
3. Dump each database â†’ /tmp/backup_TIMESTAMP/
4. Generate SHA256 checksums
5. Check if backup already exists for today
   - If exists: Move to records/YYYY/MM/DD/olds/HH_MM/
6. Upload to Google Drive â†’ records/YYYY/MM/DD/
7. Clean temp directory
8. Prune old cloud backups (15+ days)
```

### Restore Flow

```
1. User runs restore.py
2. List cloud backups, find latest with target DB
3. Download to temp â†’ /tmp/restore_TIMESTAMP/
4. Safety backup current DB â†’ manual_backups/YYYY/MM/DD/
5. Drop and recreate database
6. Restore from SQL file (quiet mode)
7. Verify tables
8. Clean temp directory
```

---

## ğŸ“¦ Prerequisites

- **Docker & Docker Compose** (v2.0+)
- **Google Cloud Project** with Drive API enabled
- **OAuth 2.0 Credentials** (Client ID & Secret)

---

## ğŸš€ Quick Start

### 1. Clone & Configure

```bash
# Clone repository
git clone <your-repo-url>
cd shared-db

# Create environment file
cp .env.example .env
nano .env
```

### 2. Configure Environment Variables

Create `.env` file:

```bash
# PostgreSQL Configuration
POSTGRES_DB=shared_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password
POSTGRES_HOST=shared-db  # Docker service name (DO NOT CHANGE)
POSTGRES_PORT=5432

# pgAdmin Configuration
PGADMIN_EMAIL=admin@example.com
PGADMIN_PASSWORD=admin_secure_password

# Backup Configuration
BACKUP_RETENTION_DAYS=15
RCLONE_REMOTE=grdive:  # Default remote name
```

### 3. Setup Google Drive (rclone)

```bash
# Start container
docker-compose up -d pgbackup

# Configure rclone interactively
docker-compose exec pgbackup rclone config

# Follow prompts:
# - Name: grdive
# - Storage: drive
# - Client ID: <your-client-id>
# - Client Secret: <your-client-secret>
# - Scope: drive
# - Root folder ID: <your-folder-id>  (optional, recommended)
# - Use headless mode if no browser available
```

**Get Google Drive Folder ID:**
1. Create folder in Google Drive: "PostgreSQL Backups"
2. Open folder, copy ID from URL: `https://drive.google.com/drive/folders/1qCTRvCtm...`
3. Use this ID in rclone config as `root_folder_id`

### 4. Start Services

```bash
# Build and start all services
docker-compose up -d

# Check logs
docker-compose logs -f pgbackup

# Verify services
docker-compose ps
```

---

## âš™ï¸ Configuration

### Docker Compose Services

| Service | Port | Description |
|---------|------|-------------|
| `db` | 5432 (localhost only) | PostgreSQL 17 database |
| `pgadmin` | 9090 (localhost only) | Web-based DB management UI |
| `pgbackup` | - | Backup/restore automation container |

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `POSTGRES_DB` | `shared_db` | Default database name |
| `POSTGRES_USER` | `postgres` | Database superuser |
| `POSTGRES_PASSWORD` | - | **Required**: Database password |
| `POSTGRES_HOST` | `shared-db` | Docker service name (hardcoded in scripts) |
| `POSTGRES_PORT` | `5432` | Database port |
| `BACKUP_RETENTION_DAYS` | `15` | Days to keep backups in cloud |
| `RCLONE_REMOTE` | `grdive:` | rclone remote name |

---

## ğŸ“– Usage

### Create New Database

```bash
# Interactive mode
docker-compose exec pgbackup python /app/mkdb.py

# Non-interactive mode
docker-compose exec pgbackup python /app/mkdb.py my_django_db
```

### Manual Backup

```bash
# Trigger backup manually
docker-compose exec pgbackup python /app/backup.py

# Check cloud storage
docker-compose exec pgbackup rclone lsf grdive:records/ --recursive
```

### Restore Database

```bash
# Restore from latest backup
docker-compose exec pgbackup python /app/restore.py my_django_db

# Restore from specific date (accepts YYYY-MM-DD or YYYY/MM/DD)
docker-compose exec pgbackup python /app/restore.py my_django_db 2025-10-07
docker-compose exec pgbackup python /app/restore.py my_django_db 2025/10/7

# Skip safety backup (not recommended)
docker-compose exec pgbackup python /app/restore.py my_django_db --skip-safety-backup
```

### Access pgAdmin

1. Open browser: `http://localhost:9090`
2. Login with credentials from `.env`
3. Add server:
   - Host: `shared-db`
   - Port: `5432`
   - User: `POSTGRES_USER`
   - Password: `POSTGRES_PASSWORD`

### Check Backup Status

```bash
# View backup logs (via docker logs)
docker-compose logs pgbackup

# Follow logs in real-time
docker-compose logs -f pgbackup

# Check cloud backups
docker-compose exec pgbackup rclone ls grdive:records/2025/10/7/

# Check manual backups
docker-compose exec pgbackup rclone ls grdive:manual_backups/

# Check cron status
docker-compose exec pgbackup pgrep crond
```

---

## ğŸ“ File Structure

```
shared-db/
â”œâ”€â”€ docker-compose.yml          # Orchestration configuration
â”œâ”€â”€ .env                        # Environment variables (NEVER commit!)
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ rclone/
â”‚   â””â”€â”€ rclone.conf             # rclone OAuth tokens (NEVER commit!)
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ Dockerfile              # Backup container image
    â”œâ”€â”€ requirements.txt        # Python dependencies
    â”œâ”€â”€ backup.py               # Automated daily backup script
    â”œâ”€â”€ restore.py              # Database restore script
    â””â”€â”€ mkdb.py                 # Quick database creation utility
```

### Cloud Storage Structure

```
Google Drive (or S3):
â”œâ”€â”€ records/                    # Daily automated backups
â”‚   â””â”€â”€ 2025/
â”‚       â””â”€â”€ 10/
â”‚           â””â”€â”€ 7/
â”‚               â”œâ”€â”€ shared_db.sql              â† Latest backup (restore uses this)
â”‚               â”œâ”€â”€ my_django_db.sql
â”‚               â”œâ”€â”€ SHA256SUMS
â”‚               â””â”€â”€ olds/                      â† Previous same-day backups
â”‚                   â”œâ”€â”€ 02_00/                 â† 02:00 backup
â”‚                   â”‚   â”œâ”€â”€ shared_db.sql
â”‚                   â”‚   â”œâ”€â”€ my_django_db.sql
â”‚                   â”‚   â””â”€â”€ SHA256SUMS
â”‚                   â””â”€â”€ 14_30/                 â† 14:30 backup
â”‚                       â”œâ”€â”€ shared_db.sql
â”‚                       â”œâ”€â”€ my_django_db.sql
â”‚                       â””â”€â”€ SHA256SUMS
â”‚
â””â”€â”€ manual_backups/             # Safety backups before restore
    â””â”€â”€ 2025/
        â””â”€â”€ 10/
            â””â”€â”€ 7/
                â”œâ”€â”€ shared_db_before_restore_05-19-21.sql
                â””â”€â”€ my_django_db_before_restore_12-30-45.sql
```

---

## ğŸ”§ Troubleshooting

### Issue: "Google verification process not completed"

**Solution:**
1. Go to Google Cloud Console â†’ OAuth consent screen
2. Add your email as "Test user"
3. Or publish the app (for production)

### Issue: "File not found: ." after setting root_folder_id

**Solution:**
- Ensure you're using only the folder ID, not the full URL
- Correct: `1qCTRvCtmfOM9h6851P8aDzb-TrQYUwf0`
- Wrong: `https://drive.google.com/drive/folders/1qCTRvCtm...`

### Issue: "ERROR: Failed to save config after 10 tries"

**Solution:**
- Ensure rclone folder is mounted as directory, not single file
- Check `docker-compose.yml`: `- ./rclone:/root/.config/rclone`

### Issue: Backup container not starting

**Solution:**
```bash
# Check logs
docker-compose logs pgbackup

# Rebuild container
docker-compose up -d --build pgbackup

# Verify cron is running
docker-compose exec pgbackup pgrep crond
```

### Issue: Database connection refused

**Solution:**
```bash
# Check if DB is healthy
docker-compose ps

# Check DB logs
docker-compose logs db

# Verify network connectivity
docker-compose exec pgbackup ping shared-db
```

### Issue: Restore shows too many system tables

**Solution:**
- This is normal! The `\dt` command only shows public schema tables
- System tables are in other schemas and won't be shown

---

## ğŸš€ Advanced

### Multiple Same-Day Backups

The system automatically handles multiple backups on the same day:

**Scenario:**
1. **02:00 AM** - Cron runs, backup created at `records/2025/10/7/`
2. **02:30 PM** - Manual backup triggered

**Result:**
```
records/2025/10/7/
â”œâ”€â”€ shared_db.sql           â† 14:30 backup (latest)
â”œâ”€â”€ test_db.sql
â””â”€â”€ olds/
    â””â”€â”€ 02_00/              â† 02:00 backup (archived)
        â”œâ”€â”€ shared_db.sql
        â””â”€â”€ test_db.sql
```

**Behavior:**
- Latest backup always at root: `records/YYYY/MM/DD/`
- Previous backups moved to: `records/YYYY/MM/DD/olds/HH_MM/`
- Restore always uses latest (root level)
- Old backups preserved for manual recovery if needed

### Connect Django Project

```yaml
# In your Django project's docker-compose.yml
services:
  web:
    # ... your config
    depends_on:
      - db
    environment:
      DATABASE_URL: postgresql://postgres:password@shared-db:5432/my_django_db
    networks:
      - shared-db-net

networks:
  shared-db-net:
    external: true
    name: shared-db-net
```

### Migrate to S3

Update `.env`:
```bash
RCLONE_REMOTE=s3backup:
```

Configure rclone:
```bash
docker-compose exec pgbackup rclone config
# Select: s3 (Amazon S3)
# Provider: AWS
# Enter credentials
```

### Custom Backup Schedule

Edit `scripts/Dockerfile`:
```dockerfile
# Change cron schedule (current: 02:00 daily)
RUN echo '0 2 * * * python /app/backup.py >> /backups/backup.log 2>&1' > /etc/crontabs/root

# Examples:
# Every 6 hours: '0 */6 * * *'
# Every Sunday 03:00: '0 3 * * 0'
# Twice daily: '0 2,14 * * *'
```

Rebuild container:
```bash
docker-compose up -d --build pgbackup
```

### Change Retention Period

Update `.env`:
```bash
BACKUP_RETENTION_DAYS=30  # Keep for 30 days
```

Restart container:
```bash
docker-compose restart pgbackup
```

### Monitor Backup Size

```bash
# Check total cloud storage
docker-compose exec pgbackup rclone size grdive:

# Check specific backup
docker-compose exec pgbackup rclone size grdive:records/2025/10/7/

# Check archived backups
docker-compose exec pgbackup rclone lsf grdive:records/2025/10/7/olds/ --recursive

# List largest backups
docker-compose exec pgbackup rclone ls grdive:records/ --recursive | sort -k1 -n -r | head -10
```

### Backup Single Database Manually

```bash
# Create temp directory
docker-compose exec pgbackup mkdir -p /tmp/manual_dump

# Dump specific database
docker-compose exec pgbackup pg_dump \
  -h shared-db \
  -p 5432 \
  -U postgres \
  -d my_django_db \
  -f /tmp/manual_dump/my_django_db.sql

# Upload to custom path
docker-compose exec pgbackup rclone copy \
  /tmp/manual_dump/my_django_db.sql \
  grdive:custom_backups/

# Cleanup
docker-compose exec pgbackup rm -rf /tmp/manual_dump
```

---

## ğŸ“Š Maintenance

### Update PostgreSQL Version

```yaml
# docker-compose.yml
services:
  db:
    image: postgres:18  # Update version
```

**Warning:** Always backup before major version upgrades!

### Update Python Dependencies

```bash
# Update requirements.txt
docker-compose exec pgbackup pip list --outdated

# Rebuild container
docker-compose up -d --build pgbackup
```

### Rotate rclone Token

```bash
# Reconfigure rclone
docker-compose exec pgbackup rclone config reconnect grdive:

# Or delete and recreate
docker-compose exec pgbackup rclone config delete grdive
docker-compose exec pgbackup rclone config
```

---

## ğŸ” Security Notes

- âš ï¸ **NEVER commit** `.env` or `rclone/rclone.conf` to Git
- âœ… Use strong passwords for `POSTGRES_PASSWORD` and `PGADMIN_PASSWORD`
- âœ… Keep OAuth tokens secure in `rclone.conf`
- âœ… Limit Google Drive access using `root_folder_id`
- âœ… Use localhost binding for exposed ports (already configured)
- âœ… Regularly rotate database passwords
- âœ… Monitor cloud storage quotas

---

## ğŸ“ License

This project is provided as-is for educational and production use.

---

## ğŸ¤ Contributing

Contributions welcome! Please follow these guidelines:

1. Test changes locally first
2. Update README if adding features
3. Follow existing code style
4. Document environment variables

---

## ğŸ“ Support

For issues or questions:
1. Check [Troubleshooting](#-troubleshooting) section
2. Review Docker logs: `docker-compose logs`
3. Verify environment configuration
4. Test rclone connectivity: `rclone lsd grdive:`

---

**Built with â¤ï¸ for reliable PostgreSQL backups**

